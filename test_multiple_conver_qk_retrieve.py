import torch
from modeling_llama_qk_retrieve import LlamaForCausalLM
#from modeling_llama import LlamaForCausalLM
from transformers import AutoTokenizer

# âœ… åŠ è½½æ¨¡å‹å’Œ tokenizer
model_path = "/mnt/sda1/Meta-Llama-3-8B-Instruct"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, block_size = 16, topk = 2).cuda()
#model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()
model.eval()

# âœ… ä»…é¢„è®¾ç”¨æˆ·è¾“å…¥å†…å®¹ï¼ˆç”¨æˆ·è½®ï¼‰

# 1: [1, 29871, 30406, 31229, 30383, 30275, 30356, 31030, 30415, 31615, 233, 159, 178, 30257, 30415, 30956, 30909, 232, 150, 173, 30755, 13, 31931, 30880, 30383, 232, 139, 155, 233, 156, 150, 232, 182, 179]

# 2: [1, 29871, 30406, 31229, 30383, 30275, 30356, 31030, 30415, 31615, 233, 159, 178, 30257, 30415, 231, 190, 131, 31882, 30594, 31974, 30886, 31071, 13, 31931, 30880, 30383, 232, 139, 155, 233, 156, 150, 232]

user_inputs = [
    "ä»‹ç»ä¸€ä¸‹Transformer",
    "ä»‹ç»çŸ©é˜µè®¡ç®—åŠ é€ŸæŠ€æœ¯",
    "ä»‹ç»ä¸¤è€…çš„å…³ç³»",
    "ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ä½äºå“ªé‡Œ",
    "ä»‹ç»è¿™åº§åŸå¸‚",
]

# âœ… åˆå§‹åŒ–å¯¹è¯å†å²
dialog_history = []
past_key_values = None

# âœ… æ„é€  prompt çš„å·¥å…·å‡½æ•°
def build_prompt(dialog):
    prompt = ""
    maintain_history = False
    if maintain_history:
        for role, text in dialog:
            prefix = "ç”¨æˆ·ï¼š" if role == "ç”¨æˆ·" else "åŠ©æ‰‹ï¼š"
            prompt += f"{prefix}{text}\n"
    else:
        cnt = 0
        for role, text in dialog:
            cnt = cnt+1
            if cnt >= len(dialog):
                prefix = "ç”¨æˆ·ï¼š" if role == "ç”¨æˆ·" else "åŠ©æ‰‹ï¼š"
                prompt += f"{prefix}{text}\n"
    prompt += "åŠ©æ‰‹ï¼š"  # å½“å‰è®©åŠ©æ‰‹ç»§ç»­å›å¤
    return prompt


# âœ… å¤šè½®å¯¹è¯ç”Ÿæˆæµ‹è¯•
for round_idx, user_input in enumerate(user_inputs):
    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    dialog_history.append(("ç”¨æˆ·", user_input))
    
    # æ„é€  promptï¼ˆå¯é€‰ä¼˜åŒ–ï¼šä»…è¿½åŠ ç”¨æˆ·è¾“å…¥ + KV cacheï¼‰
    prompt = build_prompt(dialog_history)
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(model.device)

    # ç”ŸæˆåŠ©æ‰‹å›å¤ï¼ˆåŠ¨æ€ï¼‰
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,
            #temperature = 1,
            return_dict_in_generate=True,
            output_scores=True,
        )

    # æå–ç”Ÿæˆå†…å®¹
    generated_tokens = output.sequences[0][inputs["input_ids"].shape[-1]:]
    assistant_reply = tokenizer.decode(generated_tokens, skip_special_tokens=False)


    print("\n=======================================================================")
    print(f"ğŸ§  ç¬¬ {round_idx + 1} è½®")
    print("=======================================================================\n")
    print("---------------------------------------------------------------------------------------")
    print(f"ğŸ‘¤ ç”¨æˆ·ï¼š{user_input}")
    print("---------------------------------------------------------------------------------------\n")
    print("---------------------------------------------------------------------------------------")
    print(f"prompt ï¼š{inputs}")
    print("---------------------------------------------------------------------------------------\n")
    print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
    print(f"ğŸ¤– åŠ©æ‰‹ï¼š{assistant_reply}")
    print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n")


    # å°†åŠ©æ‰‹å›å¤åŠ¨æ€åŠ å…¥å¯¹è¯å†å²ï¼ˆä¾›ä¸‹ä¸€è½®æ„é€  promptï¼‰
    dialog_history.append(("åŠ©æ‰‹", assistant_reply))
    
    
